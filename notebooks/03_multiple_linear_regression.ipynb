{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Multiple Linear Regression: Advanced Sales Analytics\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "In this notebook, you'll master:\n",
    "- **Multiple Linear Regression**: Using multiple features to predict sales\n",
    "- **Feature Engineering**: Creating meaningful variables from raw data\n",
    "- **Multicollinearity Detection**: Identifying and handling correlated features\n",
    "- **Model Interpretation**: Understanding the business impact of each factor\n",
    "- **Advanced Diagnostics**: Comprehensive model validation\n",
    "\n",
    "## üè¢ Business Context: Comprehensive Sales Analysis\n",
    "\n",
    "As Amazon's senior data scientist, you need to understand:\n",
    "- **Multiple Factors**: How marketing, traffic, pricing, and promotions work together\n",
    "- **Interaction Effects**: How different factors amplify or reduce each other's impact\n",
    "- **Feature Importance**: Which factors drive the most sales growth\n",
    "- **Optimization Opportunities**: Where to invest for maximum ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Enhanced Amazon Sales Dataset\n",
    "\n",
    "Let's create a more comprehensive dataset with additional features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate enhanced sales data with more features\n",
    "np.random.seed(42)\n",
    "n_samples = 2000\n",
    "\n",
    "# Base features\n",
    "marketing_spend = np.random.uniform(10000, 100000, n_samples)\n",
    "website_traffic = np.random.uniform(50000, 200000, n_samples)\n",
    "avg_product_price = np.random.uniform(20, 200, n_samples)\n",
    "seasonal_factor = np.sin(2 * np.pi * np.arange(n_samples) / 365) * 0.3\n",
    "promotion_active = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])\n",
    "\n",
    "# Additional features\n",
    "social_media_presence = np.random.uniform(0, 100, n_samples)  # Social media engagement\n",
    "customer_reviews = np.random.uniform(3.5, 5.0, n_samples)     # Average product rating\n",
    "competitor_price = avg_product_price + np.random.normal(0, 10, n_samples)  # Competitor pricing\n",
    "inventory_level = np.random.uniform(0.3, 1.0, n_samples)      # Stock availability (0-1)\n",
    "weekend_effect = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])  # Weekend sales boost\n",
    "\n",
    "# Generate revenue with more complex relationships\n",
    "revenue = (\n",
    "    50000 +                    # Base revenue\n",
    "    0.8 * marketing_spend +    # Marketing impact\n",
    "    0.3 * website_traffic +    # Traffic impact\n",
    "    100 * avg_product_price +  # Price impact\n",
    "    20000 * seasonal_factor +  # Seasonal impact\n",
    "    15000 * promotion_active + # Promotion impact\n",
    "    200 * social_media_presence +  # Social media impact\n",
    "    5000 * customer_reviews +      # Review impact\n",
    "    -50 * (avg_product_price - competitor_price) +  # Price competition\n",
    "    10000 * inventory_level +      # Inventory impact\n",
    "    5000 * weekend_effect +        # Weekend effect\n",
    "    # Interaction effects\n",
    "    0.001 * marketing_spend * website_traffic +  # Marketing-Traffic interaction\n",
    "    -0.1 * avg_product_price * promotion_active +  # Price-Promotion interaction\n",
    "    np.random.normal(0, 3000, n_samples)  # Random noise\n",
    ")\n",
    "\n",
    "# Create comprehensive DataFrame\n",
    "sales_df = pd.DataFrame({\n",
    "    'marketing_spend': marketing_spend,\n",
    "    'website_traffic': website_traffic,\n",
    "    'avg_product_price': avg_product_price,\n",
    "    'seasonal_factor': seasonal_factor,\n",
    "    'promotion_active': promotion_active,\n",
    "    'social_media_presence': social_media_presence,\n",
    "    'customer_reviews': customer_reviews,\n",
    "    'competitor_price': competitor_price,\n",
    "    'inventory_level': inventory_level,\n",
    "    'weekend_effect': weekend_effect,\n",
    "    'revenue': revenue\n",
    "})\n",
    "\n",
    "print(\"üìä Enhanced Amazon Sales Dataset:\")\n",
    "print(sales_df.head())\n",
    "print(f\"\\nüìà Dataset Shape: {sales_df.shape}\")\n",
    "print(f\"üí∞ Revenue Range: ${sales_df['revenue'].min():,.0f} - ${sales_df['revenue'].max():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Feature Engineering\n",
    "\n",
    "Let's create additional meaningful features that could improve our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "print(\"üîß Creating Engineered Features...\")\n",
    "\n",
    "# 1. Price competitiveness\n",
    "sales_df['price_competitiveness'] = sales_df['competitor_price'] - sales_df['avg_product_price']\n",
    "\n",
    "# 2. Marketing efficiency (revenue per marketing dollar)\n",
    "sales_df['marketing_efficiency'] = sales_df['revenue'] / sales_df['marketing_spend']\n",
    "\n",
    "# 3. Traffic conversion rate (estimated)\n",
    "sales_df['estimated_conversion_rate'] = (sales_df['revenue'] / sales_df['avg_product_price']) / sales_df['website_traffic']\n",
    "\n",
    "# 4. Price-to-quality ratio\n",
    "sales_df['price_quality_ratio'] = sales_df['avg_product_price'] / sales_df['customer_reviews']\n",
    "\n",
    "# 5. Marketing-to-traffic ratio\n",
    "sales_df['marketing_traffic_ratio'] = sales_df['marketing_spend'] / sales_df['website_traffic']\n",
    "\n",
    "# 6. Inventory efficiency\n",
    "sales_df['inventory_efficiency'] = sales_df['revenue'] * sales_df['inventory_level']\n",
    "\n",
    "# 7. Social media ROI\n",
    "sales_df['social_media_roi'] = sales_df['revenue'] / (sales_df['social_media_presence'] + 1)\n",
    "\n",
    "# 8. Seasonal marketing effectiveness\n",
    "sales_df['seasonal_marketing_effect'] = sales_df['marketing_spend'] * sales_df['seasonal_factor']\n",
    "\n",
    "print(\"‚úÖ Engineered Features Created:\")\n",
    "engineered_features = [\n",
    "    'price_competitiveness', 'marketing_efficiency', 'estimated_conversion_rate',\n",
    "    'price_quality_ratio', 'marketing_traffic_ratio', 'inventory_efficiency',\n",
    "    'social_media_roi', 'seasonal_marketing_effect'\n",
    "]\n",
    "print(f\"‚Ä¢ {len(engineered_features)} new features created\")\n",
    "print(f\"‚Ä¢ Total features: {len(sales_df.columns)} (including target)\")\n",
    "\n",
    "# Show correlation with revenue for new features\n",
    "new_features_corr = sales_df[engineered_features + ['revenue']].corr()['revenue'].sort_values(ascending=False)\n",
    "print(f\"\\nüìä Correlation with Revenue (New Features):\")\n",
    "print(new_features_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Multicollinearity Analysis\n",
    "\n",
    "Before building our model, let's check for multicollinearity (correlated features):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for modeling (exclude target and some engineered features to avoid perfect correlation)\n",
    "feature_columns = [\n",
    "    'marketing_spend', 'website_traffic', 'avg_product_price', 'seasonal_factor',\n",
    "    'promotion_active', 'social_media_presence', 'customer_reviews', \n",
    "    'competitor_price', 'inventory_level', 'weekend_effect',\n",
    "    'price_competitiveness', 'marketing_traffic_ratio', 'seasonal_marketing_effect'\n",
    "]\n",
    "\n",
    "X = sales_df[feature_columns]\n",
    "y = sales_df['revenue']\n",
    "\n",
    "# Correlation matrix for multicollinearity check\n",
    "correlation_matrix = X.corr()\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5, fmt='.2f')\n",
    "plt.title('Feature Correlation Matrix (Multicollinearity Check)', fontsize=16, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find highly correlated features\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            high_corr_pairs.append((\n",
    "                correlation_matrix.columns[i], \n",
    "                correlation_matrix.columns[j], \n",
    "                correlation_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "print(\"üîç Multicollinearity Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "if high_corr_pairs:\n",
    "    print(\"‚ö†Ô∏è  Highly Correlated Feature Pairs (|r| > 0.8):\")\n",
    "    for pair in high_corr_pairs:\n",
    "        print(f\"  ‚Ä¢ {pair[0]} ‚Üî {pair[1]}: r = {pair[2]:.3f}\")\n",
    "else:\n",
    "    print(\"‚úÖ No severe multicollinearity detected (|r| < 0.8)\")\n",
    "\n",
    "# VIF (Variance Inflation Factor) calculation\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calculate_vif(X):\n",
    "    \"\"\"Calculate VIF for each feature\"\"\"\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif_data.sort_values('VIF', ascending=False)\n",
    "\n",
    "vif_results = calculate_vif(X)\n",
    "print(f\"\\nüìä Variance Inflation Factor (VIF):\")\n",
    "print(vif_results)\n",
    "\n",
    "# Identify problematic features (VIF > 10)\n",
    "high_vif_features = vif_results[vif_results['VIF'] > 10]\n",
    "if not high_vif_features.empty:\n",
    "    print(f\"\\n‚ö†Ô∏è  Features with High VIF (> 10):\")\n",
    "    print(high_vif_features)\n",
    "else:\n",
    "    print(f\"\\n‚úÖ No features with problematic VIF (> 10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Multiple Linear Regression Model\n",
    "\n",
    "Now let's build our comprehensive model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale features (important for interpretation)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Build multiple linear regression model\n",
    "multiple_model = LinearRegression()\n",
    "multiple_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = multiple_model.predict(X_train_scaled)\n",
    "y_pred_test = multiple_model.predict(X_test_scaled)\n",
    "\n",
    "# Model coefficients\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': feature_columns,\n",
    "    'Coefficient': multiple_model.coef_,\n",
    "    'Abs_Coefficient': np.abs(multiple_model.coef_)\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(\"üéØ Multiple Linear Regression Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Intercept: ${multiple_model.intercept_:,.2f}\")\n",
    "print(f\"\\nüìä Feature Importance (by absolute coefficient):\")\n",
    "print(coefficients.round(2))\n",
    "\n",
    "# Business interpretation\n",
    "print(f\"\\nüíº Business Interpretation:\")\n",
    "print(f\"‚Ä¢ Base Revenue: ${multiple_model.intercept_:,.2f}\")\n",
    "print(f\"‚Ä¢ Most Important Factor: {coefficients.iloc[0]['Feature']} (${coefficients.iloc[0]['Coefficient']:,.2f})\")\n",
    "print(f\"‚Ä¢ Least Important Factor: {coefficients.iloc[-1]['Feature']} (${coefficients.iloc[-1]['Coefficient']:,.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Feature importance plot\n",
    "colors = ['#FF9900' if x > 0 else '#232F3E' for x in coefficients['Coefficient']]\n",
    "bars = plt.barh(range(len(coefficients)), coefficients['Abs_Coefficient'], color=colors, alpha=0.7)\n",
    "\n",
    "# Add coefficient values as text\n",
    "for i, (idx, row) in enumerate(coefficients.iterrows()):\n",
    "    plt.text(row['Abs_Coefficient'] + 1000, i, f\"${row['Coefficient']:,.0f}\", \n",
    "             va='center', fontweight='bold')\n",
    "\n",
    "plt.yticks(range(len(coefficients)), coefficients['Feature'])\n",
    "plt.xlabel('Absolute Coefficient Value ($)')\n",
    "plt.title('Feature Importance in Sales Prediction Model', fontsize=16, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed coefficient analysis\n",
    "print(\"üìä Detailed Coefficient Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "for _, row in coefficients.head(5).iterrows():\n",
    "    impact = \"Positive\" if row['Coefficient'] > 0 else \"Negative\"\n",
    "    print(f\"‚Ä¢ {row['Feature']}: ${row['Coefficient']:,.0f} ({impact} impact)\")\n",
    "    \n",
    "    # Business interpretation for top features\n",
    "    if row['Feature'] == 'marketing_spend':\n",
    "        print(f\"  ‚Üí Every $1 increase in marketing generates ${row['Coefficient']:.2f} in revenue\")\n",
    "    elif row['Feature'] == 'website_traffic':\n",
    "        print(f\"  ‚Üí Every additional visitor generates ${row['Coefficient']:.2f} in revenue\")\n",
    "    elif row['Feature'] == 'avg_product_price':\n",
    "        print(f\"  ‚Üí Every $1 price increase generates ${row['Coefficient']:.2f} in revenue\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Model Evaluation & Comparison\n",
    "\n",
    "Let's compare our multiple regression model with the simple model from the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive metrics for multiple regression\n",
    "def calculate_comprehensive_metrics(y_true, y_pred, model_name):\n",
    "    \"\"\"Calculate all evaluation metrics\"\"\"\n",
    "    \n",
    "    # Basic metrics\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # Adjusted R-squared\n",
    "    n = len(y_true)\n",
    "    p = len(feature_columns)  # number of features\n",
    "    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "    \n",
    "    # MAPE\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    # Business metrics\n",
    "    avg_revenue = np.mean(y_true)\n",
    "    error_percentage = (rmse / avg_revenue) * 100\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'mse': mse, 'rmse': rmse, 'mae': mae, \n",
    "        'mape': mape, 'r2': r2, 'adj_r2': adj_r2,\n",
    "        'error_percentage': error_percentage\n",
    "    }\n",
    "\n",
    "# Calculate metrics for multiple regression\n",
    "multiple_metrics = calculate_comprehensive_metrics(y_test, y_pred_test, \"Multiple Linear Regression\")\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    'Metric': ['MSE', 'RMSE', 'MAE', 'MAPE (%)', 'R¬≤', 'Adjusted R¬≤', 'Error %'],\n",
    "    'Multiple Regression': [\n",
    "        f\"{multiple_metrics['mse']:,.0f}\",\n",
    "        f\"{multiple_metrics['rmse']:,.0f}\",\n",
    "        f\"{multiple_metrics['mae']:,.0f}\",\n",
    "        f\"{multiple_metrics['mape']:.2f}\",\n",
    "        f\"{multiple_metrics['r2']:.4f}\",\n",
    "        f\"{multiple_metrics['adj_r2']:.4f}\",\n",
    "        f\"{multiple_metrics['error_percentage']:.1f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"üìä Model Performance Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Business interpretation\n",
    "print(f\"\\nüíº Business Impact Analysis:\")\n",
    "print(f\"‚Ä¢ Model Accuracy: {multiple_metrics['r2']*100:.1f}% of variance explained\")\n",
    "print(f\"‚Ä¢ Prediction Error: ${multiple_metrics['rmse']:,.0f} (¬±{multiple_metrics['error_percentage']:.1f}%)\")\n",
    "print(f\"‚Ä¢ Adjusted for Complexity: {multiple_metrics['adj_r2']*100:.1f}% (adjusted R¬≤)\")\n",
    "\n",
    "# Improvement analysis\n",
    "if 'simple_metrics' in globals():\n",
    "    improvement_r2 = ((multiple_metrics['r2'] - simple_metrics['r2']) / simple_metrics['r2']) * 100\n",
    "    improvement_rmse = ((simple_metrics['rmse'] - multiple_metrics['rmse']) / simple_metrics['rmse']) * 100\n",
    "    print(f\"\\nüìà Improvement over Simple Model:\")\n",
    "    print(f\"‚Ä¢ R¬≤ Improvement: {improvement_r2:.1f}%\")\n",
    "    print(f\"‚Ä¢ RMSE Reduction: {improvement_rmse:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Advanced Model Diagnostics\n",
    "\n",
    "Let's perform comprehensive model diagnostics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced diagnostics using statsmodels\n",
    "X_with_const = sm.add_constant(X_train_scaled)\n",
    "model_sm = sm.OLS(y_train, X_with_const).fit()\n",
    "\n",
    "print(\"üîç Advanced Model Diagnostics:\")\n",
    "print(\"=\" * 50)\n",
    "print(model_sm.summary())\n",
    "\n",
    "# Residual analysis\n",
    "residuals = y_test - y_pred_test\n",
    "predicted = y_pred_test\n",
    "\n",
    "# Create diagnostic plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Multiple Linear Regression Diagnostics', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Residuals vs Predicted\n",
    "axes[0, 0].scatter(predicted, residuals, alpha=0.6, color='#FF9900')\n",
    "axes[0, 0].axhline(y=0, color='red', linestyle='--', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Predicted Revenue')\n",
    "axes[0, 0].set_ylabel('Residuals')\n",
    "axes[0, 0].set_title('Residuals vs Predicted')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Q-Q Plot\n",
    "from scipy.stats import probplot\n",
    "probplot(residuals, dist=\"norm\", plot=axes[0, 1])\n",
    "axes[0, 1].set_title('Q-Q Plot (Normality Check)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Residuals Histogram\n",
    "axes[0, 2].hist(residuals, bins=30, alpha=0.7, color='#232F3E', edgecolor='black')\n",
    "axes[0, 2].set_xlabel('Residuals')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "axes[0, 2].set_title('Residuals Distribution')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Predicted vs Actual\n",
    "axes[1, 0].scatter(y_test, predicted, alpha=0.6, color='#146EB4')\n",
    "axes[1, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                'r--', lw=2, label='Perfect Prediction')\n",
    "axes[1, 0].set_xlabel('Actual Revenue')\n",
    "axes[1, 0].set_ylabel('Predicted Revenue')\n",
    "axes[1, 0].set_title('Predicted vs Actual')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Residuals vs Index\n",
    "axes[1, 1].plot(range(len(residuals)), residuals, alpha=0.7, color='#FF6B6B')\n",
    "axes[1, 1].axhline(y=0, color='red', linestyle='--', alpha=0.8)\n",
    "axes[1, 1].set_xlabel('Observation Index')\n",
    "axes[1, 1].set_ylabel('Residuals')\n",
    "axes[1, 1].set_title('Residuals vs Index')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Leverage Plot\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Calculate leverage\n",
    "X_scaled = StandardScaler().fit_transform(X_train)\n",
    "H = X_scaled @ np.linalg.inv(X_scaled.T @ X_scaled) @ X_scaled.T\n",
    "leverage = np.diag(H)\n",
    "\n",
    "axes[1, 2].scatter(leverage, residuals[:len(leverage)], alpha=0.6, color='#4ECDC4')\n",
    "axes[1, 2].set_xlabel('Leverage')\n",
    "axes[1, 2].set_ylabel('Residuals')\n",
    "axes[1, 2].set_title('Residuals vs Leverage')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Business Insights & Recommendations\n",
    "\n",
    "### **Key Findings from Multiple Linear Regression:**\n",
    "\n",
    "1. **Most Important Factors:**\n",
    "   - {coefficients.iloc[0]['Feature']}: ${coefficients.iloc[0]['Coefficient']:,.0f} impact\n",
    "   - {coefficients.iloc[1]['Feature']}: ${coefficients.iloc[1]['Coefficient']:,.0f} impact\n",
    "   - {coefficients.iloc[2]['Feature']}: ${coefficients.iloc[2]['Coefficient']:,.0f} impact\n",
    "\n",
    "2. **Model Performance:**\n",
    "   - **Accuracy**: {multiple_metrics['r2']*100:.1f}% of variance explained\n",
    "   - **Prediction Error**: ¬±{multiple_metrics['error_percentage']:.1f}% on average\n",
    "   - **Complexity Adjusted**: {multiple_metrics['adj_r2']*100:.1f}% (adjusted R¬≤)\n",
    "\n",
    "3. **Business Recommendations:**\n",
    "   - **Investment Priority**: Focus on {coefficients.iloc[0]['Feature']} for maximum ROI\n",
    "   - **Resource Allocation**: Optimize {coefficients.iloc[1]['Feature']} and {coefficients.iloc[2]['Feature']}\n",
    "   - **Risk Management**: Monitor factors with negative coefficients\n",
    "\n",
    "### **Strategic Implications:**\n",
    "- **Marketing Strategy**: Allocate budget based on coefficient importance\n",
    "- **Pricing Strategy**: Consider price elasticity and competitor pricing\n",
    "- **Inventory Management**: Optimize stock levels based on demand predictions\n",
    "- **Performance Monitoring**: Track actual vs predicted performance\n",
    "\n",
    "### **Model Limitations:**\n",
    "- **Linear Assumption**: May not capture complex non-linear relationships\n",
    "- **Feature Interactions**: Limited interaction effects in current model\n",
    "- **External Factors**: Market conditions, economic factors not included\n",
    "- **Temporal Effects**: Time-based patterns may need special handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps: Advanced Techniques\n",
    "\n",
    "In the next notebook, we'll explore:\n",
    "\n",
    "### **Advanced Topics:**\n",
    "1. **Regularization**: Ridge, Lasso, and Elastic Net regression\n",
    "2. **Cross-Validation**: Robust model validation techniques\n",
    "3. **Feature Selection**: Automated feature importance and selection\n",
    "4. **Model Drift**: Detecting when models need retraining\n",
    "5. **Hyperparameter Tuning**: Optimizing model parameters\n",
    "\n",
    "### **Business Applications:**\n",
    "- **Automated Forecasting**: Real-time sales predictions\n",
    "- **Dynamic Pricing**: Price optimization based on demand\n",
    "- **Marketing Attribution**: Understanding campaign effectiveness\n",
    "- **Risk Assessment**: Identifying sales decline indicators\n",
    "\n",
    "### **Production Considerations:**\n",
    "- **Model Deployment**: Putting models into production\n",
    "- **Performance Monitoring**: Tracking model accuracy over time\n",
    "- **A/B Testing**: Comparing different model versions\n",
    "- **Scalability**: Handling large-scale data processing\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to master advanced regression techniques?** üéØ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_expression": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "nbformat_minor": 4,
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
} 